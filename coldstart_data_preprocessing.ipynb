{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOypc4s5tdZiNlmNIJxiRas"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ104DUWnuBV"
      },
      "outputs": [],
      "source": [
        "# Your Python code here\n",
        "!pip install transformers accelerate peft bitsandbytes datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJZXg7ySn0Qj",
        "outputId": "09d77158-92dc-43c4-a523-ff76aac5f25a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coldstart_path = input(\"Enter the full path where the coldstart data should be saved (e.g., mydrive/coldstart_data): \")\n",
        "os.environ[\"COLDSTART_FOLDER_PATH\"] = coldstart_path\n",
        "coldstart_file_path = os.getenv(\"COLDSTART_FOLDER_PATH\") #  Coldstart file path ffor initial train data"
      ],
      "metadata": {
        "id": "RSQQ45bUoLyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, load_dataset, Dataset\n",
        "\n",
        "# Define file paths based on the coldstart_file_path\n",
        "train_file = os.path.join(coldstart_file_path, \"train.jsonl\")\n",
        "valid_file = os.path.join(coldstart_file_path, \"valid.jsonl\")\n",
        "test_file = os.path.join(coldstart_file_path, \"test.jsonl\")\n",
        "\n",
        "# checking the dataset format\n",
        "with open(train_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    for i in range(5):  # Read first 5 lines\n",
        "        print(file.readline().strip())  # Print each line"
      ],
      "metadata": {
        "id": "54mL9qzLoU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the dataset format\n",
        "with open(valid_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    for i in range(5):  # Read first 5 lines\n",
        "        print(file.readline().strip())  # Print each line"
      ],
      "metadata": {
        "id": "F82EO-4_vePS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the dataset format\n",
        "with open(test_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    for i in range(5):  # Read first 5 lines\n",
        "        print(file.readline().strip())  # Print each line"
      ],
      "metadata": {
        "id": "INvgytK_vgqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to load JSONL file into a Hugging Face dataset\n",
        "def load_jsonl_as_dataset(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = [json.loads(line.strip()) for line in file]  # Load all lines efficiently\n",
        "    return Dataset.from_list(data)\n"
      ],
      "metadata": {
        "id": "opqYr9HWpIWj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = load_jsonl_as_dataset(train_file)\n",
        "dataset_valid = load_jsonl_as_dataset(valid_file)\n",
        "dataset_test = load_jsonl_as_dataset(test_file)"
      ],
      "metadata": {
        "id": "i3KxiA8_H2aG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset structure\n",
        "print(dataset_train, \"\\n\", dataset_valid, \"\\n\", dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6Di6erpWkD",
        "outputId": "e2f2ed48-9f7e-40e8-b053-e0f2f92b324f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 382\n",
            "}) \n",
            " Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 14\n",
            "}) \n",
            " Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 26\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset_train)"
      ],
      "metadata": {
        "id": "-FWP0W2iplUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_valid[\"text\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibgikF2ypmg2",
        "outputId": "a316483a-a744-4d5a-bd6e-a836dfc5d4f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "Hi, how are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hello! I’m doing well, thanks for asking. How can I help you today?<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_qa_from_message(message):\n",
        "    \"\"\"\n",
        "    Extracts the user question and assistant answer from a given message.\n",
        "    \"\"\"\n",
        "    # Regex pattern to match <|im_start|>user and <|im_start|>assistant segments\n",
        "    user_pattern = r\"<\\|im_start\\|>user\\s*(.*?)<\\|im_end\\|>\"\n",
        "    assistant_pattern = r\"<\\|im_start\\|>assistant\\s*(.*?)<\\|im_end\\|>\"\n",
        "\n",
        "    # Extract question and answer using regex\n",
        "    user_match = re.search(user_pattern, message, re.DOTALL)\n",
        "    assistant_match = re.search(assistant_pattern, message, re.DOTALL)\n",
        "\n",
        "    # Get extracted text or None\n",
        "    question = user_match.group(1).strip() if user_match else None\n",
        "    answer = assistant_match.group(1).strip() if assistant_match else None\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "def create_qa_dataset(messages):\n",
        "    \"\"\"\n",
        "    Processes a list of messages and creates a dataset with 'question' and 'answer' columns.\n",
        "    \"\"\"\n",
        "    questions, answers = [], []\n",
        "\n",
        "    for message in messages:\n",
        "        question, answer = extract_qa_from_message(message)\n",
        "        if question and answer:  # Ensure both exist\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "    # Convert to Hugging Face Dataset format\n",
        "    dataset = Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "0FdxWK4zprnA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for creating dataset with question and answer columns\n",
        "\n",
        "def process_messages_to_qa_dataset(messages):\n",
        "    \"\"\"\n",
        "    Extracts questions and answers from a list of messages and returns a structured dataset.\n",
        "    \"\"\"\n",
        "    questions, answers = [], []\n",
        "\n",
        "    for message in messages:\n",
        "        # Regex patterns to extract user and assistant messages\n",
        "        user_match = re.search(r\"<\\|im_start\\|>user\\s*(.*?)<\\|im_end\\|>\", message, re.DOTALL)\n",
        "        assistant_match = re.search(r\"<\\|im_start\\|>assistant\\s*(.*?)<\\|im_end\\|>\", message, re.DOTALL)\n",
        "\n",
        "        # Extracted content\n",
        "        question = user_match.group(1).strip() if user_match else None\n",
        "        answer = assistant_match.group(1).strip() if assistant_match else None\n",
        "\n",
        "        if question and answer:  # Ensure valid Q&A pairs\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "    # Convert extracted data to a Hugging Face Dataset\n",
        "    return Dataset.from_dict({\"question\": questions, \"answer\": answers})\n"
      ],
      "metadata": {
        "id": "dTEl5Txhpx8y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset_train = process_messages_to_qa_dataset(dataset_train[\"text\"])\n",
        "qa_dataset_valid = process_messages_to_qa_dataset(dataset_valid[\"text\"])\n",
        "qa_dataset_test = process_messages_to_qa_dataset(dataset_test[\"text\"])"
      ],
      "metadata": {
        "id": "e1h6H-0Op7hQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_dataset_train.column_names)\n",
        "print(qa_dataset_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KElysbnQp_dz",
        "outputId": "614875d9-2e5a-4770-d1f4-932507aa4d5d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['question', 'answer']\n",
            "{'question': 'If a car travels 60 miles in 2 hours, what is its average speed?', 'answer': '<think>Average speed is distance divided by time: 60 miles ÷ 2 hours = 30 miles per hour.</think>\\n<answer>30 miles per hour.</answer>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the save path\n",
        "save_path = input(\"Enter the full path where the modified coldstart csv should be saved: \")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "pd.DataFrame(qa_dataset_train).to_csv(save_path + \"coldstart_dataset_train.csv\", index=False)\n",
        "pd.DataFrame(qa_dataset_valid).to_csv(save_path + \"coldstart_dataset_valid.csv\", index=False)\n",
        "pd.DataFrame(qa_dataset_test).to_csv(save_path + \"coldstart_dataset_test.csv\", index=False)\n",
        "\n",
        "print(f\"Datasets saved to {save_path}\")\n"
      ],
      "metadata": {
        "id": "3E0ajnkkLLOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}